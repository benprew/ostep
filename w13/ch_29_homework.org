** Questions
1. *Weâ€™ll start by redoing the measurements within this chapter. Use the call `gettimeofday()` to measure time within your program. How accurate is this timer? What is the smallest interval it can measure? Gain confidence in its workings, as we will need it in all subsequent questions. You can also look into other timers, such as the cycle counter available on x86 via the rdtsc instruction.*
2. *Now, build a simple concurrent counter and measure how long it takes to increment the counter many times as the number of threads increases. How many CPUs are available on the system you are using? Does this number impact your measurements at all?*

   I didn't get the sharp increase the author's did, even after increasing the iterations to 10 million per thread.
   #+begin_src shell
     for i in {1..10}; do ./simple $i; done
     Time taken: 0 sec [threads=1,total=10000000]
     Time taken: 1 sec [threads=2,total=20000000]
     Time taken: 1 sec [threads=3,total=30000000]
     Time taken: 2 sec [threads=4,total=40000000]
     Time taken: 4 sec [threads=5,total=50000000]
     Time taken: 4 sec [threads=6,total=60000000]
     Time taken: 7 sec [threads=7,total=70000000]
     Time taken: 8 sec [threads=8,total=80000000]
     Time taken: 11 sec [threads=9,total=90000000]
     Time taken: 9 sec [threads=10,total=100000000]
   #+end_src

   It's interesting to see the command take multiple seconds of user and system time, but wallclock is much less. (threads run across cores)
   #+begin_src shell
     /usr/bin/time -v ./simple 10
     Time taken: 9 sec [threads=10,total=100000000]
     Command being timed: "./simple 10"
     User time (seconds): 21.00
     System time (seconds): 37.51
     Percent of CPU this job got: 709%
     Elapsed (wall clock) time (h:mm:ss or m:ss): 0:08.24
   #+end_src

   And without threads it runs even faster!
   #+begin_src shell
     for i in {1..10}; do ./simple $i 0; done
     Time taken: 0 sec [threads=0,total=10000000]
     Time taken: 0 sec [threads=0,total=20000000]
     Time taken: 1 sec [threads=0,total=30000000]
     Time taken: 0 sec [threads=0,total=40000000]
     Time taken: 0 sec [threads=0,total=50000000]
     Time taken: 1 sec [threads=0,total=60000000]
     Time taken: 0 sec [threads=0,total=70000000]
     Time taken: 1 sec [threads=0,total=80000000]
     Time taken: 1 sec [threads=0,total=90000000]
     Time taken: 0 sec [threads=0,total=100000000]
   #+end_src

   #+begin_src shell
     /usr/bin/time -v ./simple 10 0
     Time taken: 1 sec [threads=0,total=100000000]
     Command being timed: "./simple 10 0"
     User time (seconds): 0.72
     System time (seconds): 0.00
     Percent of CPU this job got: 99%
     Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.73
   #+end_src

3. *Next, build a version of the sloppy counter. Once again, measure its performance as the number of threads varies, as well as the threshold. Do the numbers match what you see in the chapter?*

   I don't see any improvement over the simple counter and with a default threshold of 256, it's actually slower
   #+begin_src shell
     for i in {1..10}; do ./sloppy $i 256; done
     Time taken: 0 sec [threads=1,total=9999872]
     Time taken: 1 sec [threads=2,total=20000000]
     Time taken: 2 sec [threads=3,total=29999872]
     Time taken: 2 sec [threads=4,total=39999744]
     Time taken: 3 sec [threads=5,total=49999872]
     Time taken: 3 sec [threads=6,total=59999744]
     Time taken: 5 sec [threads=7,total=69999616]
     Time taken: 7 sec [threads=8,total=79999744]
     Time taken: 9 sec [threads=9,total=89999616]
     Time taken: 9 sec [threads=10,total=99999744]
   #+end_src
   Even with a very high threshold, I don't see much of an improvement.  I'm guessing context switching is enough to cause performance issues?
   #+begin_src shell
     for i in {1..10}; do ./sloppy $i 900000; done
     Time taken: 0 sec [threads=1,threshold=900000,total=9900000]
     Time taken: 2 sec [threads=2,threshold=900000,total=19800000]
     Time taken: 1 sec [threads=3,threshold=900000,total=29700000]
     Time taken: 2 sec [threads=4,threshold=900000,total=39600000]
     Time taken: 3 sec [threads=5,threshold=900000,total=49500000]
     Time taken: 5 sec [threads=6,threshold=900000,total=59400000]
     Time taken: 6 sec [threads=7,threshold=900000,total=69300000]
     Time taken: 8 sec [threads=8,threshold=900000,total=79200000]
     Time taken: 8 sec [threads=9,threshold=900000,total=89100000]
     Time taken: 10 sec [threads=10,threshold=900000,total=99000000]
   #+end_src
4. *Build a version of a linked list that uses hand-over-hand locking [MS04], as cited in the chapter. You should read the paper first to understand how it works, and then implement it. Measure its performance. When does a hand-over-hand list work better than a standard list as shown in the chapter?*
5. *Pick your favorite data structure, such as a B-tree or other slightly more interesting structure. Implement it, and start with a simple locking strategy such as a single lock. Measure its performance as the number of concurrent threads increases.*
6. *Finally, think of a more interesting locking strategy for this favorite data structure of yours. Implement it, and measure its performance. How does it compare to the straightforward locking approach?*
